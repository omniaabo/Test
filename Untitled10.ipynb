{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omniaabo/Test/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cblGtXcYA5ME"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from operator import length_hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj0hD4_NYzLT"
      },
      "source": [
        "# **Shortest** **Path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G3OWfzkPV5gD"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "#shortest path\n",
        "\"\"\"\n",
        "Created on Sun Aug  7 14:11:56 2022\n",
        "\n",
        "@author: hp\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        " \n",
        "class Graph(object):\n",
        "    def __init__(self, nodes, init_graph):\n",
        "        self.nodes = nodes\n",
        "        self.graph = self.construct_graph(nodes, init_graph)\n",
        "        \n",
        "    def construct_graph(self, nodes, init_graph):\n",
        "        \n",
        "        #This method makes sure that the graph is symmetrical. In other words, if there's a path from node A to B with a value V, there needs to be a path from node B to node A with a value V.\n",
        "       \n",
        "        graph = {}\n",
        "        for node in nodes:\n",
        "            graph[node] = {}\n",
        "        \n",
        "        graph.update(init_graph)\n",
        "        \n",
        "        for node, edges in graph.items():\n",
        "            for adjacent_node, value in edges.items():\n",
        "                if graph[adjacent_node].get(node, False) == False:\n",
        "                    graph[adjacent_node][node] = value\n",
        "                    \n",
        "        return graph\n",
        "    \n",
        "    def get_nodes(self):\n",
        "        \"Returns the nodes of the graph.\"\n",
        "        return self.nodes\n",
        "    \n",
        "    def get_outgoing_edges(self, node):\n",
        "        \"Returns the neighbors of a node.\"\n",
        "        connections = []\n",
        "        for out_node in self.nodes:\n",
        "            if self.graph[node].get(out_node, False) != False:\n",
        "                connections.append(out_node)\n",
        "        return connections\n",
        "    \n",
        "    def value(self, node1, node2):\n",
        "        \"Returns the value of an edge between two nodes.\"\n",
        "        return self.graph[node1][node2]\n",
        "\n",
        "\n",
        "def dijkstra_algorithm(graph, start_node):\n",
        "    unvisited_nodes = list(graph.get_nodes())\n",
        " \n",
        "    # We'll use this dict to save the cost of visiting each node and update it as we move along the graph   \n",
        "    shortest_path = {}\n",
        " \n",
        "    # We'll use this dict to save the shortest known path to a node found so far\n",
        "    previous_nodes = {}\n",
        " \n",
        "    # We'll use max_value to initialize the \"infinity\" value of the unvisited nodes   \n",
        "    max_value = sys.maxsize\n",
        "    for node in unvisited_nodes:\n",
        "        shortest_path[node] = max_value\n",
        "    # However, we initialize the starting node's value with 0   \n",
        "    shortest_path[start_node] = 0\n",
        "    count=0\n",
        "    # The algorithm executes until we visit all nodes\n",
        "    while unvisited_nodes:\n",
        "        # The code block below finds the node with the lowest score\n",
        "        current_min_node = None\n",
        "        for node in unvisited_nodes: # Iterate over the nodes\n",
        "            if current_min_node == None:\n",
        "                current_min_node = node\n",
        "            elif shortest_path[node] < shortest_path[current_min_node]:\n",
        "                current_min_node = node\n",
        "                \n",
        "        # The code block below retrieves the current node's neighbors and updates their distances\n",
        "        neighbors = graph.get_outgoing_edges(current_min_node)\n",
        "        for neighbor in neighbors:\n",
        "            tentative_value = shortest_path[current_min_node] + graph.value(current_min_node, neighbor)\n",
        "            if tentative_value < shortest_path[neighbor]:\n",
        "                shortest_path[neighbor] = tentative_value\n",
        "                # We also update the best path to the current node\n",
        "                previous_nodes[neighbor] = current_min_node\n",
        " \n",
        "        # After visiting its neighbors, we mark the node as \"visited\"\n",
        "        unvisited_nodes.remove(current_min_node)\n",
        "    \n",
        "    return previous_nodes, shortest_path\n",
        "\n",
        "def print_result(previous_nodes, shortest_path, start_node, target_node):\n",
        "    path = []\n",
        "    node = target_node\n",
        "    \n",
        "    while node != start_node:\n",
        "        path.append(node)\n",
        "        node = previous_nodes[node]\n",
        " \n",
        "    # Add the start node manually\n",
        "    path.append(start_node)\n",
        "    \n",
        "    #print(\"We found the following best path with a value of {}.\".format(shortest_path[target_node]))\n",
        "    #print(\" -> \".join(reversed(path)))\n",
        "    #print(f\"Number Of Hops = {len(path)}\")\n",
        "    return format(shortest_path[target_node]),len(path)\n",
        "\n",
        "def ShortestPath(start,end):\n",
        "    nodes = [\"ATLAM5\", \"ATLAng\", \"CHINng\", \"DNVRng\", \"HSTNng\", \"IPLSng\", \"KSCYng\", \"LOSAng\", \"NYCMng\", \"SNVAng\", \"STTLng\", \"WASHng\"]\n",
        "    \n",
        "     \n",
        "    init_graph = {}\n",
        "    for node in nodes:\n",
        "        init_graph[node] = {}\n",
        "        \n",
        "    init_graph[\"ATLAM5\"][\"ATLAng\"] = 0.6618242212428414\n",
        "    init_graph[\"ATLAng\"][\"HSTNng\"] = 5.395735790861154\n",
        "    init_graph[\"ATLAng\"][\"IPLSng\"] = 2.950379471305515\n",
        "    init_graph[\"ATLAng\"][\"WASHng\"] = 4.496181432177612\n",
        "    init_graph[\"CHINng\"][\"IPLSng\"] = 1.2954971045263282\n",
        "    init_graph[\"CHINng\"][\"NYCMng\"] = 5.724346541223465\n",
        "    init_graph[\"DNVRng\"][\"KSCYng\"] = 3.720073798163767               \n",
        "    init_graph[\"DNVRng\"][\"SNVAng\"] = 7.569993244146227               \n",
        "    init_graph[\"DNVRng\"][\"STTLng\"] = 7.854873360987624\n",
        "    init_graph[\"HSTNng\"][\"KSCYng\"] = 5.134158303429244\n",
        "    init_graph[\"HSTNng\"][\"LOSAng\"] = 10.964808201671724               \n",
        "    init_graph[\"IPLSng\"][\"KSCYng\"] = 4.506317299225467\n",
        "    init_graph[\"LOSAng\"][\"SNVAng\"] = 2.518247042266586\n",
        "    init_graph[\"NYCMng\"][\"WASHng\"] = 1.674941676397201\n",
        "    init_graph[\"SNVAng\"][\"STTLng\"] = 5.6799681106984 \n",
        "    \n",
        "    '''\n",
        "    #init_graph[\"Moscow\"][\"Belgrade\"] = 5\n",
        "    #init_graph[\"Moscow\"][\"Athens\"] = 4\n",
        "    #init_graph[\"Athens\"][\"Belgrade\"] = 1\n",
        "    #init_graph[\"Rome\"][\"Berlin\"] = 2\n",
        "    #init_graph[\"Rome\"][\"Athens\"] = 2\n",
        "    '''\n",
        "    graph = Graph(nodes, init_graph)\n",
        "    previous_nodes, shortest_path = dijkstra_algorithm(graph=graph, start_node= start)\n",
        "    result = print_result(previous_nodes, shortest_path, start_node= start, target_node= end)\n",
        "   \n",
        "    return result\n",
        "\n",
        "def BestControllerLocation(lista):\n",
        "    BestPath = 200.9999999 ###########################################\n",
        "    for i in lista:\n",
        "        global nwl\n",
        "        global ControllerLocation\n",
        "        global Hops\n",
        "        global PropagationDelay\n",
        "        totalweight = 0\n",
        "        elementsweight = []\n",
        "        delay=[]\n",
        "        newlista = lista.copy()\n",
        "        newlista.remove(i)\n",
        "        count=[]\n",
        "        for j in newlista:\n",
        "            w,c= ShortestPath(i,j)\n",
        "            totalweight+=float(w)\n",
        "            count.append(c)\n",
        "            elementsweight.append(j)\n",
        "            delay.append(float(w))\n",
        "        if float(totalweight) < float(BestPath):\n",
        "            BestPath = totalweight\n",
        "            ControllerLocation = i\n",
        "            nwl = elementsweight\n",
        "            PropagationDelay=delay\n",
        "            Hops=count\n",
        "        #print(lista.index(i))\n",
        "        #print(BestPath)\n",
        "    # print(f\"nwl = {nwl}\")\n",
        "    nwl.insert(0, ControllerLocation)\n",
        "    Hops.insert(0, 1)\n",
        "    PropagationDelay.insert(0, 0)\n",
        "    #print(Hops[2])\n",
        "    #print(PropagationDelay[2])\n",
        "    return nwl,Hops,PropagationDelay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OteoOuPvZEyN"
      },
      "source": [
        "# **Reward** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NnwcGMd6V_Z4"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "#Reward\n",
        "\"\"\"\n",
        "Created on Wed May 11 14:54:00 2022\n",
        "\n",
        "@author: hp\n",
        "\"\"\"\n",
        "import math\n",
        "import pandas as pd\n",
        "#from shortest_path import BestControllerLocation\n",
        "\n",
        "Switches_names={0:\"ATLAM5\",1:\"ATLAng\",2:\"CHINng\",3:\"DNVRng\",4:\"HSTNng\",5:\"IPLSng\",6:\"KSCYng\",7:\"LOSAng\",8:\"NYCMng\",\n",
        "                9:\"SNVAng\",10:\"STTLng\",11:\"WASHng\"}     \n",
        "Controllers_names={12:\"c1\",13:\"c2\",14:\"c3\"}\n",
        "\n",
        "Switch_Controller = [(9,12),(6,13),(5,14)]\n",
        "pathes={0:[0,1,5,14], 1:[1,5,14], 2:[2,5,14], 3:[3,6,13], 4:[4,6,13], 5:[5,14],\n",
        "         6:[6,13], 7:[7,9,12], 8:[8,2,5,14], 9:[9,12], 10:[10,9,12], 11:[11,8,2,5,14]}\n",
        "\n",
        "Mapping = pd.DataFrame.from_dict(pathes, orient='index').transpose()\n",
        "\n",
        "listOfLongAndLatit = {0: (-84.3833, 33.75), 1: (-85.5, 34.5), 2: (-87.6167, 41.8333), \n",
        " 3: (-105.0, 40.75), 4: (-95.517364, 29.770031), 5: (-86.159535, 39.780622), \n",
        " 6: (-96.596704, 38.961694), 7: (-118.25, 34.05), 8: (-73.9667, 40.7833), 9: (-122.02553, 37.38575), \n",
        " 10: (-122.3, 47.6), 11: (-77.026842, 38.897303), 12:(-120.85851, 39.67858333),\n",
        " 13:(-99.03802267, 36.49390833), 14:(-82.4421795, 38.25742083)}\n",
        "\n",
        "Switchestraff = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6:7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12}\n",
        "#listOfLongAndLatit = [(-84.3833,33.75),(-85.5,34.5),(-95.517364,29.770031)]\n",
        "#listOfLongAndLatit.reverse() \n",
        "Bmax = 50000\n",
        "class Reward:\n",
        "    \n",
        "    def __init__(self,hops,PropagationDelay):\n",
        "        self.hops = hops\n",
        "        self.PropagationDelay = PropagationDelay\n",
        "        #self.listOfLongAndLatit = listOfLongAndLatit\n",
        "        #self.NumOfSwitches = NumOfSwitches\n",
        "        #self.Mapping = Mapping\n",
        "        #self.NumOfControllers = NumOfControllers\n",
        "    \n",
        "    def processing(self):\n",
        "        processing_time = 0\n",
        "        Controller_processing_latency =0.03\n",
        "        Switch_processing_latency = 0.2\n",
        "        for x in range(self.hops):\n",
        "            processing_time +=Switch_processing_latency\n",
        "            \n",
        "        processing_time+=Controller_processing_latency\n",
        "        return processing_time\n",
        "        \n",
        "    def forwarding(self):\n",
        "        Forwarding_latency =0.2\n",
        "        forwarding_time = 0\n",
        "        for x in range(self.hops):\n",
        "            forwarding_time +=Forwarding_latency\n",
        "        return forwarding_time\n",
        "    \n",
        "    #def propagation(self):\n",
        "     #   return self.PropagationDelay\n",
        "            \n",
        "   \n",
        "    def RoundTripTime(self):\n",
        "        Trtt = 2*(self.processing()+self.forwarding()+self.PropagationDelay)\n",
        "        return Trtt\n",
        "class Flows:\n",
        "\n",
        "    #def PacketInAverageLatency(self,SwitchesInCluster,SwitchesControllerpathes,Switchestraffic):\n",
        "    def PacketInAverageLatency(path,hops,PropagationDelay,traffic):\n",
        "        TotalFlows = 0\n",
        "        Latency = 0\n",
        "        for i in range(len(hops)):\n",
        "            r = Reward(hops[i],PropagationDelay[i])\n",
        "            #print(traffic[i])\n",
        "      \n",
        "            Latency +=  r.RoundTripTime() * np.float_(traffic[i])\n",
        "            #print(Latency)\n",
        "            TotalFlows+=sum(traffic[i])\n",
        "            #print(TotalFlows)\n",
        "            #ALatency = Latency/TotalFlows\n",
        "            #print(ALatency)\n",
        "        ALatency = Latency/TotalFlows\n",
        "        return ALatency,TotalFlows\n",
        "                \n",
        "    def LoadBalance(action,ClustersNumber,state):\n",
        "        Switches_names={0:\"ATLAM5\",1:\"ATLAng\",2:\"CHINng\",3:\"DNVRng\",4:\"HSTNng\",5:\"IPLSng\",6:\"KSCYng\",7:\"LOSAng\",8:\"NYCMng\",\n",
        "                9:\"SNVAng\",10:\"STTLng\",11:\"WASHng\"}  \n",
        "        TotalALatency = 0\n",
        "        TotalFlowFluctuations = 0\n",
        "        count = 0\n",
        "        DeltaB =0\n",
        "        FlowsOfLoad = []\n",
        "        cluster = {}\n",
        "        flows=Flows\n",
        "        #reward = 0\n",
        "        for i in action:\n",
        "          #print(f\"the value of action is {i}\")\n",
        "          cluster.update(i)\n",
        "        sorted_dict = sorted(cluster.items(), key=lambda kv:(kv[1], kv[0]))    \n",
        "        cluster.clear()\n",
        "        for i,j in sorted_dict:\n",
        "            if j==count:\n",
        "                cluster.update({i:j})\n",
        "            else:\n",
        "                Path,hops,PropagationDelay = BestControllerLocation(list(cluster.keys()))\n",
        "                #for i in range(len(hops)):\n",
        "                l=[]\n",
        "                for i in Path:\n",
        "                    l.append(list(state[list(Switches_names.keys())[list(Switches_names.values()).index(i)]].values()))\n",
        "                ALatency,totalflows=flows.PacketInAverageLatency(Path,hops,PropagationDelay,l)\n",
        "                l.clear() \n",
        "                #ALatency = latency/totalflows\n",
        "                if totalflows > Bmax:\n",
        "                    return 0.1,0.1\n",
        "                FlowsOfLoad.append(totalflows)\n",
        "                TotalALatency+=ALatency\n",
        "                TotalFlowFluctuations+=totalflows\n",
        "                cluster.clear()\n",
        "                cluster.update({i:j})\n",
        "                count+=1\n",
        "        Path,hops,PropagationDelay = BestControllerLocation(list(cluster.keys()))\n",
        "        #for count in range(len(hops)):\n",
        "        l=[]\n",
        "        for i in Path:\n",
        "            l.append(list(state[list(Switches_names.keys())[list(Switches_names.values()).index(i)]].values()))           \n",
        "       \n",
        "        ALatency,totalflows=flows.PacketInAverageLatency(Path,hops,PropagationDelay,l)\n",
        "        l.clear()\n",
        "        #ALatency = latency/totalflows\n",
        "        if totalflows > Bmax:\n",
        "            return 0.1,0.1\n",
        "        FlowsOfLoad.append(totalflows)\n",
        "        TotalALatency+=ALatency\n",
        "        TotalFlowFluctuations+=totalflows\n",
        "        Lavg = TotalALatency/ClustersNumber\n",
        "        Bavg = TotalFlowFluctuations/ClustersNumber\n",
        "        for i in FlowsOfLoad:\n",
        "            DeltaB +=(i-Bavg)**2\n",
        "        DeltaB = math.sqrt(DeltaB/ClustersNumber)\n",
        "        return  Lavg,DeltaB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ObN4lE9MtP"
      },
      "source": [
        "#* **Dictionary to Tensor** *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NBpsBsw3wqZ7",
        "outputId": "894c6b96-b29b-477b-fc8f-942e45248df7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nr=[]\\ni=0\\n#for i in range(3):\\ndef testy(c,b):\\n    i = c%b\\n    r.append([])\\n    r[i].append(DictToTensorList(state))\\n    i+=1\\n   \\ntesty(3,3)    \\nprint(r[0])\\ntesty(3,2)\\nprint(r[1])\\nprint(r)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Dictionary to Tensor List\n",
        "# state=[{1:2},{1:5},{2:2},{2:4}]\n",
        "def DictToTensorList(state):\n",
        "    keys=[]\n",
        "    values=[]\n",
        "    for i in state:\n",
        "        for a,b in i.items():\n",
        "            keys.append(a)\n",
        "            values.append(b)\n",
        "    ls1 = T.Tensor([keys,values])  \n",
        "    # print(f\"ls1 = {ls1.H.flatten().tolist()}\")  \n",
        "    Tensor2D = ls1.H.flatten().tolist()\n",
        "    #print(Tensor2D.flatten().tolist())\n",
        "    TensotToList = ls1.H.squeeze().tolist()\n",
        "    return Tensor2D\n",
        "# print(DictToTensorList(state))\n",
        "# observation =[{0: 9.314551}, {1: 151.188115}, {1: 131.828622}, {1: 124.998645},\n",
        "#              {1: 159.46923}, {0: 324.026864}, {2: 87.957416}, {2: 328.540483}, \n",
        "#              {2: 461.294549}, {2: 33.413244}, {2: 121.985259}, {1: 607.703116}]\n",
        "# for i in range(0,12):\n",
        "#   partOfObsrevation = DictToTensorList(observation[i:i+1])\n",
        "#   print(partOfObsrevation)\n",
        "'''\n",
        "r=[]\n",
        "i=0\n",
        "#for i in range(3):\n",
        "def testy(c,b):\n",
        "    i = c%b\n",
        "    r.append([])\n",
        "    r[i].append(DictToTensorList(state))\n",
        "    i+=1\n",
        "   \n",
        "testy(3,3)    \n",
        "print(r[0])\n",
        "testy(3,2)\n",
        "print(r[1])\n",
        "print(r)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DictToTwoTensorList**"
      ],
      "metadata": {
        "id": "5B1hyrsxVBQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state=[{1:2},{1:5},{2:2},{2:4}]\n",
        "def DictToTwoTensorList(state):\n",
        "    # keys=[]\n",
        "    # values=[]\n",
        "    ls1 = []\n",
        "    for i in state:\n",
        "        for a,b in i.items():\n",
        "          ls1.append([a,b])\n",
        "    print(f\"ls1 = {ls1}\")  \n",
        "    # Tensor2D = ls1.H.flatten().tolist()\n",
        "    #print(Tensor2D.flatten().tolist())\n",
        "    # TensotToList = ls1.H.squeeze().tolist()\n",
        "    return ls1\n",
        "print(DictToTensorList(state))\n",
        "print(DictToTwoTensorList(state))\n",
        "# observation =[{0: 9.314551}, {1: 151.188115}, {1: 131.828622}, {1: 124.998645},\n",
        "#              {1: 159.46923}, {0: 324.026864}, {2: 87.957416}, {2: 328.540483}, \n",
        "#              {2: 461.294549}, {2: 33.413244}, {2: 121.985259}, {1: 607.703116}]\n",
        "# for i in range(0,12):\n",
        "#   partOfObsrevation = DictToTensorList(observation[i:i+1])\n",
        "#   print(partOfObsrevation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o4B7JKXVDf_",
        "outputId": "4c146a2d-cdfe-4bfd-e9a5-a61e3130d6a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 2.0, 1.0, 5.0, 2.0, 2.0, 2.0, 4.0]\n",
            "ls1 = [[1, 2], [1, 5], [2, 2], [2, 4]]\n",
            "[[1, 2], [1, 5], [2, 2], [2, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jchUeIAbfjb"
      },
      "source": [
        "#* **MultiList Dictionary to Tensor** *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pFLw7dFobgWM"
      },
      "outputs": [],
      "source": [
        "#MultiList Dictionary to Tensor \n",
        "def MultiListDictToTensorList(state):\n",
        "    keys=[]\n",
        "    values=[]\n",
        "    for i in state:\n",
        "      for j in i:\n",
        "        for a,b in j.items():\n",
        "            keys.append(a)\n",
        "            values.append(b)\n",
        "    ls1 = T.Tensor([keys,values])    \n",
        "    Tensor2D = ls1.H\n",
        "    #print(Tensor2D.flatten().tolist())\n",
        "    TensotToList = ls1.H.squeeze().tolist()\n",
        "    return Tensor2D.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctcQ2G4PsBHW"
      },
      "source": [
        "#* **Dictionary to Tensor Action** *\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AvM8mwPBQM6k"
      },
      "outputs": [],
      "source": [
        "#Dictionary to Tensor Action\n",
        "Switches_names={0:\"ATLAM5\",1:\"ATLAng\",2:\"CHINng\",3:\"DNVRng\",4:\"HSTNng\",5:\"IPLSng\",6:\"KSCYng\",7:\"LOSAng\",8:\"NYCMng\",\n",
        "                9:\"SNVAng\",10:\"STTLng\",11:\"WASHng\"}\n",
        "action_ddqn = [{'ATLAM5': 0}, {'ATLAng': 0}, {'CHINng': 0}, {'DNVRng': 0}, {'HSTNng': 0}, {'IPLSng': 0}, {'KSCYng': 0}, {'LOSAng': 0}, {'NYCMng': 0}, {'SNVAng': 0}, {'STTLng': 0}, {'WASHng': 0}]\n",
        "def DictToTensorAction(action):\n",
        "    # print(f\"DictToTensorAction--> {action}\")\n",
        "    keys = []\n",
        "    values = []\n",
        "    names = {}\n",
        "    for i in action:\n",
        "        for a,b in i.items():\n",
        "            keys.append(a)\n",
        "            values.append(b)\n",
        "    df = pd.DataFrame(keys,columns=['k'])\n",
        "    for i in keys:\n",
        "      names.update({i:list(Switches_names.keys())[list(Switches_names.values()).index(i)]})\n",
        "    df['k'] = df['k'].replace(names)\n",
        "    ls1 = T.Tensor([df['k'],values])\n",
        "    Tensor2D = ls1.H\n",
        "    #print(Tensor2D.flatten().tolist())\n",
        "    TensotToList = ls1.H.squeeze().tolist()\n",
        "    return Tensor2D.flatten().tolist()\n",
        "# print(action)\n",
        "# print(DictToTensorAction(action_ddqn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LVJrCeNq8gr_",
        "outputId": "e3a14880-e24e-4470-f07d-1528d6842c68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ns=DictToTensorAction(action)\\nt= T.Tensor(s)\\nt=t.flatten().tolist()\\nw[0]=t\\nprint(w[0])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "action = [{'ATLAM5': 1}, {'ATLAng': 1}, {'CHINng': 2}, {'DNVRng': 2}, {'HSTNng': 1}, {'IPLSng': 1}, {'KSCYng': 2}, {'LOSAng': 2}, {'NYCMng': 2}, {'SNVAng': 2}, {'STTLng': 1}, {'WASHng': 0}]\n",
        "#print(DictToTensorAction(action))\n",
        "#print(action)\n",
        "'''\n",
        "s=DictToTensorAction(action)\n",
        "t= T.Tensor(s)\n",
        "t=t.flatten().tolist()\n",
        "w[0]=t\n",
        "print(w[0])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGtwZaz86myz"
      },
      "source": [
        "# **Dictionary To Two Lists**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FJrBkMnP6wCT",
        "outputId": "e871d7b2-54df-418d-e8f3-fb71bb98f667"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nx,y = DictToTwoLists(state)\\nprint(x,y)\\ns=[]\\ny=y\\nx=x\\nfor i,j in zip(y , x):\\n    s.append({i:j})\\nprint(s)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Dictionary To Two Lists\n",
        "state=[{1:2},{1:5},{2:2},{2:4}]\n",
        "def DictToTwoLists(state):\n",
        "    keys=[]\n",
        "    values=[]\n",
        "    for i in state:\n",
        "        for a,b in i.items():\n",
        "            keys.append(a)\n",
        "            values.append(b)\n",
        "    ls1 = T.Tensor(values) \n",
        "    #ls2 = T.Tensor(keys)\n",
        "    ls2 = keys\n",
        "    #Tensor2D = ls1.H\n",
        "    #print(Tensor2D.flatten().tolist())\n",
        "    #TensotToList = ls1.H.squeeze().tolist()\n",
        "    return ls1.tolist(),ls2\n",
        "'''\n",
        "x,y = DictToTwoLists(state)\n",
        "print(x,y)\n",
        "s=[]\n",
        "y=y\n",
        "x=x\n",
        "for i,j in zip(y , x):\n",
        "    s.append({i:j})\n",
        "print(s)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXERaiawZvKl"
      },
      "source": [
        "# **DQN Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Uit4AB3OaAy_",
        "outputId": "1e2e3366-7898-45c0-f8ec-c55850621d05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#os.environ[\\'CUDA_LAUNCH_BLOCKING\\'] = \"1\"\\nCUDA_LAUNCH_BLOCKING = \"1\"\\nglobal Call\\n\\nclass DeepQNetwork(nn.Module):\\n    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions):\\n        super(DeepQNetwork, self).__init__()\\n        self.input_dims = input_dims\\n        self.fc1_dims = fc1_dims\\n        self.fc2_dims = fc2_dims\\n        self.n_actions = n_actions\\n        \\n        self.fc1 = nn.Linear(2, 64)\\n        nn.init.xavier_uniform(self.fc1.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc2 = nn.Linear(64, 64)\\n        nn.init.xavier_uniform(self.fc2.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc3 = nn.Linear(64, 32)\\n        nn.init.xavier_uniform(self.fc3.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc4 = nn.Linear(32, 3)\\n        nn.init.xavier_uniform(self.fc4.weight.data, gain=nn.init.calculate_gain(\\'linear\\'))\\n        self.fc5 = nn.Linear(24, 64)\\n        nn.init.xavier_uniform(self.fc1.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc6 = nn.Linear(32, 1)\\n        nn.init.xavier_uniform(self.fc4.weight.data, gain=nn.init.calculate_gain(\\'linear\\'))\\n\\n\\n        self.fc11 = nn.Linear(24, 64)\\n        nn.init.xavier_uniform(self.fc11.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc22 = nn.Linear(64, 64)\\n        nn.init.xavier_uniform(self.fc22.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc33 = nn.Linear(64, 64)\\n        nn.init.xavier_uniform(self.fc33.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc44 = nn.Linear(64, 64)\\n        nn.init.xavier_uniform(self.fc44.weight.data, gain=nn.init.calculate_gain(\\'relu\\'))\\n        self.fc55 = nn.Linear(64, 1)\\n        nn.init.xavier_uniform(self.fc55.weight.data, gain=nn.init.calculate_gain(\\'linear\\'))\\n        self.fc66 = nn.Linear(64, 1)\\n        nn.init.xavier_uniform(self.fc66.weight.data, gain=nn.init.calculate_gain(\\'linear\\'))\\n\\n\\n        self.double()\\n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\\n        self.loss = nn.MSELoss()\\n        self.device = T.device(\\'cuda:0\\' if T.cuda.is_available() else \\'cpu\\')\\n        self.to(self.device)\\n        \\n    def forward(self,state):\\n        state = state.double()\\n        x = F.relu(self.fc1(state))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        print(x.weight)\\n        actions = self.fc4(x)\\n        return actions\\n    \\n    def forward2(self,state):\\n        x = F.relu(self.fc5(state))\\n        x = F.relu(self.fc2(x))\\n        x = F.relu(self.fc3(x))\\n        actions = self.fc6(x)\\n        return actions\\n\\n    def forward3(self,state):\\n        x = F.relu(self.fc11(state))\\n        x = F.relu(self.fc22(x))\\n        v = F.relu(self.fc33(x))\\n        a = F.relu(self.fc44(x))\\n        v = self.fc55(v)\\n        a = self.fc66(a)\\n        x = v.expand(v.size(0), 1) + a - a.mean(1).unsqueeze(1).expand(a.size(0), 1)\\n        return x\\n\\nclass Agent():\\n    def __init__(self,n,n_actions,ActionSpace,gamma,epsilon,lr,input_dims,batch_size,\\n                mem_size=100_000,eps_end=0.02,eps_dec=10_000):\\n        \\n        #self.action_sapce = [i for i in range(n_actions)]  #My Modify\\n        self.NumOfSwitches = n\\n        self.action_space = ActionSpace\\n        self.gamma = gamma\\n        self.epsilon = epsilon\\n        self.eps_dec = eps_dec\\n        self.eps_min = eps_end \\n        self.lr = lr\\n        self.mem_size = mem_size\\n        self.batch_size = batch_size\\n        self.mem_cntr = 0\\n        self.Q_eval = DeepQNetwork(self.lr,n_actions=n_actions,input_dims=input_dims,fc1_dims=64\\n                                   ,fc2_dims=64) #n_actions\\n        #self.state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\\n        #self.new_state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\\n        #self.action_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\\n        #self.reward_memory = np.zeros(self.mem_size,dtype=np.float64)\\n        self.state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\\n        self.new_state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\\n        self.action_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\\n        self.reward_memory = np.zeros(self.mem_size,dtype=np.float64)\\n        self.terminal_memory = np.zeros(self.mem_size,dtype=bool)\\n    #     self.intialization_weights()\\n    # def intialization_weights(self):\\n    #   for m in self.modules():\\n    #     print(m)   \\n    def choose_action(self,observation):\\n        if np.random.random() < self.epsilon: # 0.02 instead of np.random.random() knowing that length of done must be over than the number of choosing random actions\\n            action = randomchoice(self.action_space)\\n            #x=0\\n            #print(\" action in choose action = \"+ str(action))\\n        \\n        else:\\n            action = []\\n            #observation,x = DictToTwoLists(observation)\\n            observation = DictToTensorList(observation)#######################################################################\\n            state = T.tensor([observation]).to(self.Q_eval.device)\\n            count = 0\\n            for i in range(0,self.NumOfSwitches*2,2):############################################################################\\n              print(f\"state[0]{[i]} in choose action  = {state[0][i:i+2]}\")\\n              actions = self.Q_eval.forward(state[0][i:i+2].unsqueeze(dim= 0))###################################################\\n              print(f\"actions[0]{[i]} is {actions}\")\\n              action.append({Switches_names[count]:T.argmax(actions).item()}) #################################################\\n              print(f\"action[0]{[i]} is {action}\")  \\n              count +=1\\n            \\'\\'\\'\\n            action = []\\n            #print(observation)\\n            #state = T.tensor([observation]).to(self.Q_eval.device) # i think their will be a problem here\\n            #observation = DictToTensorList(observation)\\n            observation,x = DictToTwoLists(observation)\\n            print(\"observation in choose action = \"+str(observation.size()))\\n            state = T.tensor([observation]).to(self.Q_eval.device)\\n            print(\"state size in choose action = \"+str(state.size()))\\n            actions = self.Q_eval.forward(state)\\n            #print(\"actions.H in choose action = \"+str(actions.H))\\n            #error may occur \\n            for i in actions:\\n                action.append(T.argmax(actions).item())\\n            #print(\"len of action in choose action = \"+str(len(action)))\\n            \\'\\'\\'\\n        return action #########################################################################\\n\\n    def store_transition(self,state,action,reward,state_,done):\\n        index = self.mem_cntr % self.mem_size\\n        #print(f\"state = {state}\")\\n        #print(f\"action = {action}\")\\n        self.state_memory[index] = DictToTensorList(state)\\n        #self.state_memory[index],xs = DictToTwoLists(state)\\n        #print(f\"store_transition: state_memory length = {len(self.state_memory[index])}\")\\n        self.new_state_memory[index] = DictToTensorList(state_)\\n        #self.new_state_memory[index],xs_ = DictToTwoLists(state_)\\n        #print(f\"new_state_memory = {self.new_state_memory}\") #may a problem where the newstate and state not changed\\n        self.action_memory[index] = DictToTensorAction(action)\\n        #self.action_memory[index],xA = DictToTwoLists(action)\\n        self.reward_memory[index] = reward\\n        self.terminal_memory[index] = done\\n        self.mem_cntr +=1\\n    \\n\\n    \\n    \\n    def learn(self):\\n        if self.mem_cntr < self.batch_size:\\n            return\\n        \\n        self.Q_eval.optimizer.zero_grad()\\n\\n        max_mem = min(self.mem_cntr, self.mem_size)\\n        print(f\"learn max_mem = {max_mem}\")\\n        batch = np.random.choice(max_mem, self.batch_size, replace=False)\\n        #print(f\"learn batch = {batch}\")\\n        batch_index = np.arange(self.batch_size, dtype=np.int64)\\n        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\\n        print(\"learn state_batch size = \"+str(state_batch.size()))\\n        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\\n        action_batch = self.action_memory[batch]\\n        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\\n        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\\n        #print(f\"the state_batch size is {state_batch.size()}\")\\n        #print(f\"learn the batch_index is {batch_index}\")\\n        #print(f\"learn the action_batch is {action_batch}\")\\n        #q_eval=self.Q_eval.forward(state_batch)[batch_index, action_batch]\\n        \\n        #print(f\"learn state_batch[j]{[j]} = { state_batch[j][i:i+2]}\")\\n        q_eval = self.Q_eval.forward3(state_batch)#[batch_index,action_batch]#[0, 1]########################################################\\n        q_next = self.Q_eval.forward3(new_state_batch)#################################################################\\n        #print(f\"learn terminal_batch = {terminal_batch}\")\\n        #print(f\"learn q_next = {q_next}\")\\n        q_next[terminal_batch] = 0.0\\n        #q_target.append(reward_batch + self.gamma*T.max(q_next, dim=1)[0])###################################################\\n        q_target = reward_batch + self.gamma*T.max(q_next)\\n        #print(f\"learn q_next after = {q_next}\")\\n        #print(f\"learn q_target  = {q_target}\")\\n        \\n        #loss = self.Q_eval.loss(q_target.float(), q_eval.float()).to(self.Q_eval.device)######################################\\n        #print(f\"learn q_eval len  = {len(q_eval)}\")\\n        loss = self.Q_eval.loss(q_target,q_eval).to(self.Q_eval.device)\\n        #loss = criterion(model_prediction.float(), target_variable.float())\\n        loss.backward()########################################################################################################\\n        self.Q_eval.optimizer.step() #may should be inside the loop\\n        print(f\"loss = {loss}\")\\n        #self.iter_cntr += 1\\n        self.epsilon = self.epsilon - self.eps_dec             if self.epsilon > self.eps_min else self.eps_min\\n            \\n# Q_eval\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Jul 22 14:45:31 2022\n",
        "\n",
        "@author: hp\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "CUDA_LAUNCH_BLOCKING = \"1\"\n",
        "global Call\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        \n",
        "        self.fc1 = nn.Linear(2, 64)\n",
        "        nn.init.xavier_uniform(self.fc1.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        nn.init.xavier_uniform(self.fc2.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        nn.init.xavier_uniform(self.fc3.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc4 = nn.Linear(32, 3)\n",
        "        nn.init.xavier_uniform(self.fc4.weight.data, gain=nn.init.calculate_gain('linear'))\n",
        "        self.fc5 = nn.Linear(24, 64)\n",
        "        nn.init.xavier_uniform(self.fc1.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc6 = nn.Linear(32, 1)\n",
        "        nn.init.xavier_uniform(self.fc4.weight.data, gain=nn.init.calculate_gain('linear'))\n",
        "\n",
        "\n",
        "        self.fc11 = nn.Linear(24, 64)\n",
        "        nn.init.xavier_uniform(self.fc11.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc22 = nn.Linear(64, 64)\n",
        "        nn.init.xavier_uniform(self.fc22.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc33 = nn.Linear(64, 64)\n",
        "        nn.init.xavier_uniform(self.fc33.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc44 = nn.Linear(64, 64)\n",
        "        nn.init.xavier_uniform(self.fc44.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        self.fc55 = nn.Linear(64, 1)\n",
        "        nn.init.xavier_uniform(self.fc55.weight.data, gain=nn.init.calculate_gain('linear'))\n",
        "        self.fc66 = nn.Linear(64, 1)\n",
        "        nn.init.xavier_uniform(self.fc66.weight.data, gain=nn.init.calculate_gain('linear'))\n",
        "\n",
        "\n",
        "        self.double()\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def forward(self,state):\n",
        "        state = state.double()\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        print(x.weight)\n",
        "        actions = self.fc4(x)\n",
        "        return actions\n",
        "    \n",
        "    def forward2(self,state):\n",
        "        x = F.relu(self.fc5(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        actions = self.fc6(x)\n",
        "        return actions\n",
        "\n",
        "    def forward3(self,state):\n",
        "        x = F.relu(self.fc11(state))\n",
        "        x = F.relu(self.fc22(x))\n",
        "        v = F.relu(self.fc33(x))\n",
        "        a = F.relu(self.fc44(x))\n",
        "        v = self.fc55(v)\n",
        "        a = self.fc66(a)\n",
        "        x = v.expand(v.size(0), 1) + a - a.mean(1).unsqueeze(1).expand(a.size(0), 1)\n",
        "        return x\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self,n,n_actions,ActionSpace,gamma,epsilon,lr,input_dims,batch_size,\n",
        "                mem_size=100_000,eps_end=0.02,eps_dec=10_000):\n",
        "        \n",
        "        #self.action_sapce = [i for i in range(n_actions)]  #My Modify\n",
        "        self.NumOfSwitches = n\n",
        "        self.action_space = ActionSpace\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_dec = eps_dec\n",
        "        self.eps_min = eps_end \n",
        "        self.lr = lr\n",
        "        self.mem_size = mem_size\n",
        "        self.batch_size = batch_size\n",
        "        self.mem_cntr = 0\n",
        "        self.Q_eval = DeepQNetwork(self.lr,n_actions=n_actions,input_dims=input_dims,fc1_dims=64\n",
        "                                   ,fc2_dims=64) #n_actions\n",
        "        #self.state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\n",
        "        #self.new_state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\n",
        "        #self.action_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\n",
        "        #self.reward_memory = np.zeros(self.mem_size,dtype=np.float64)\n",
        "        self.state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\n",
        "        self.new_state_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\n",
        "        self.action_memory = np.zeros((self.mem_size,*input_dims),dtype=np.float64)\n",
        "        self.reward_memory = np.zeros(self.mem_size,dtype=np.float64)\n",
        "        self.terminal_memory = np.zeros(self.mem_size,dtype=bool)\n",
        "    #     self.intialization_weights()\n",
        "    # def intialization_weights(self):\n",
        "    #   for m in self.modules():\n",
        "    #     print(m)   \n",
        "    def choose_action(self,observation):\n",
        "        if np.random.random() < self.epsilon: # 0.02 instead of np.random.random() knowing that length of done must be over than the number of choosing random actions\n",
        "            action = randomchoice(self.action_space)\n",
        "            #x=0\n",
        "            #print(\" action in choose action = \"+ str(action))\n",
        "        \n",
        "        else:\n",
        "            action = []\n",
        "            #observation,x = DictToTwoLists(observation)\n",
        "            observation = DictToTensorList(observation)#######################################################################\n",
        "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
        "            count = 0\n",
        "            for i in range(0,self.NumOfSwitches*2,2):############################################################################\n",
        "              print(f\"state[0]{[i]} in choose action  = {state[0][i:i+2]}\")\n",
        "              actions = self.Q_eval.forward(state[0][i:i+2].unsqueeze(dim= 0))###################################################\n",
        "              print(f\"actions[0]{[i]} is {actions}\")\n",
        "              action.append({Switches_names[count]:T.argmax(actions).item()}) #################################################\n",
        "              print(f\"action[0]{[i]} is {action}\")  \n",
        "              count +=1\n",
        "            '''\n",
        "            action = []\n",
        "            #print(observation)\n",
        "            #state = T.tensor([observation]).to(self.Q_eval.device) # i think their will be a problem here\n",
        "            #observation = DictToTensorList(observation)\n",
        "            observation,x = DictToTwoLists(observation)\n",
        "            print(\"observation in choose action = \"+str(observation.size()))\n",
        "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
        "            print(\"state size in choose action = \"+str(state.size()))\n",
        "            actions = self.Q_eval.forward(state)\n",
        "            #print(\"actions.H in choose action = \"+str(actions.H))\n",
        "            #error may occur \n",
        "            for i in actions:\n",
        "                action.append(T.argmax(actions).item())\n",
        "            #print(\"len of action in choose action = \"+str(len(action)))\n",
        "            '''\n",
        "        return action #########################################################################\n",
        "\n",
        "    def store_transition(self,state,action,reward,state_,done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        #print(f\"state = {state}\")\n",
        "        #print(f\"action = {action}\")\n",
        "        self.state_memory[index] = DictToTensorList(state)\n",
        "        #self.state_memory[index],xs = DictToTwoLists(state)\n",
        "        #print(f\"store_transition: state_memory length = {len(self.state_memory[index])}\")\n",
        "        self.new_state_memory[index] = DictToTensorList(state_)\n",
        "        #self.new_state_memory[index],xs_ = DictToTwoLists(state_)\n",
        "        #print(f\"new_state_memory = {self.new_state_memory}\") #may a problem where the newstate and state not changed\n",
        "        self.action_memory[index] = DictToTensorAction(action)\n",
        "        #self.action_memory[index],xA = DictToTwoLists(action)\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr +=1\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    def learn(self):\n",
        "        if self.mem_cntr < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        print(f\"learn max_mem = {max_mem}\")\n",
        "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
        "        #print(f\"learn batch = {batch}\")\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int64)\n",
        "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
        "        print(\"learn state_batch size = \"+str(state_batch.size()))\n",
        "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
        "        action_batch = self.action_memory[batch]\n",
        "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
        "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
        "        #print(f\"the state_batch size is {state_batch.size()}\")\n",
        "        #print(f\"learn the batch_index is {batch_index}\")\n",
        "        #print(f\"learn the action_batch is {action_batch}\")\n",
        "        #q_eval=self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "        \n",
        "        #print(f\"learn state_batch[j]{[j]} = { state_batch[j][i:i+2]}\")\n",
        "        q_eval = self.Q_eval.forward3(state_batch)#[batch_index,action_batch]#[0, 1]########################################################\n",
        "        q_next = self.Q_eval.forward3(new_state_batch)#################################################################\n",
        "        #print(f\"learn terminal_batch = {terminal_batch}\")\n",
        "        #print(f\"learn q_next = {q_next}\")\n",
        "        q_next[terminal_batch] = 0.0\n",
        "        #q_target.append(reward_batch + self.gamma*T.max(q_next, dim=1)[0])###################################################\n",
        "        q_target = reward_batch + self.gamma*T.max(q_next)\n",
        "        #print(f\"learn q_next after = {q_next}\")\n",
        "        #print(f\"learn q_target  = {q_target}\")\n",
        "        \n",
        "        #loss = self.Q_eval.loss(q_target.float(), q_eval.float()).to(self.Q_eval.device)######################################\n",
        "        #print(f\"learn q_eval len  = {len(q_eval)}\")\n",
        "        loss = self.Q_eval.loss(q_target,q_eval).to(self.Q_eval.device)\n",
        "        #loss = criterion(model_prediction.float(), target_variable.float())\n",
        "        loss.backward()########################################################################################################\n",
        "        self.Q_eval.optimizer.step() #may should be inside the loop\n",
        "        print(f\"loss = {loss}\")\n",
        "        #self.iter_cntr += 1\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "            if self.epsilon > self.eps_min else self.eps_min\n",
        "            \n",
        "# Q_eval\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZsEYlz1n8pL",
        "outputId": "2bb8a8ba-4cd5-423d-cd27-4272bd00d9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learn state_batch[0] = [1, 2]\n",
            "learn state_batch[0] = [3, 4]\n",
            "learn state_batch[1] = [4, 5]\n",
            "learn state_batch[1] = [6, 7]\n",
            "learn state_batch[2] = [7, 8]\n",
            "learn state_batch[2] = [9, 10]\n"
          ]
        }
      ],
      "source": [
        "#testaya\n",
        "#print(np.random.choice(4,4,replace = False))\n",
        "#print(f\"arrange = {np.arange(4, dtype=np.int64)}\")\n",
        "test1 = T.tensor([1,2,3,4])\n",
        "test2 = T.tensor([4,8,2,3])\n",
        "#print(f\"T.max = {T.max(T.max(test1,test2))}\")\n",
        "loss = nn.MSELoss()\n",
        "device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "d1=T.tensor(1.4)#[1,2,3]\n",
        "d2=T.tensor(3)#[4,5,6]\n",
        "lossy = loss(d1.float(), d2.float()).to(device)\n",
        "\n",
        "#print(f\"lossy = {lossy}\")\n",
        "#print(f\"d1.float() = {d1.float()}\")\n",
        "s = []\n",
        "t = []\n",
        "for i in range(3):\n",
        "  s.append([])\n",
        "  s[i].append(T.tensor([i,i,i]))\n",
        "\n",
        "b = np.random.choice(3, 3, replace=False)\n",
        "l =T.tensor([1,2,3])\n",
        "#print(5 + 2*T.max(l)[0])\n",
        "state_batch = [[1,2,3,4],[4,5,6,7],[7,8,9,10]]\n",
        "for j in range(3):\n",
        "      for i in range(0,4,2): \n",
        "          print(f\"learn state_batch{[j]} = { state_batch[j][i:i+2]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9jHMTJ6a66e"
      },
      "source": [
        "# **Deuling Double DQN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p6z3xHzjbDvR"
      },
      "outputs": [],
      "source": [
        "# from torch._C import LongTensor\n",
        "from numpy.compat import long\n",
        "from pandas.io.formats.style_render import DataFrame\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                    dtype=np.float64)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float64)\n",
        "        self.action_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                    dtype=np.float64)\n",
        "        # self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float64)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = DictToTensorList(state)\n",
        "        # print(f\"DictToTensorList(state) = {DictToTensorList(state)}\")\n",
        "        self.new_state_memory[index] = DictToTensorList(state_)\n",
        "        # print(f\"action ddqn = {action}\")\n",
        "        # print(f\"DictToTensorAction(action) = {DictToTensorAction(action)}\")\n",
        "        self.action_memory[index] = DictToTensorAction(action)\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal\n",
        "\n",
        "class DuelingDeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, n_actions, input_dims, name, chkpt_dir):\n",
        "        super(DuelingDeepQNetwork, self).__init__()\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "        self.fc1 = nn.Linear(*input_dims, 512)\n",
        "        # self.fc1 = nn.Linear(24,64)\n",
        "        self.V = nn.Linear(512, 1)\n",
        "        self.A = nn.Linear(512, 24)\n",
        "        # self.A = nn.Linear(512, 3)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "        #test\n",
        "        self.fc2 = nn.Linear(2, 512)\n",
        "        self.Ao = nn.Linear(512, 3)\n",
        "        self.double()\n",
        "    def forward(self, state):\n",
        "        flat1 = F.relu(self.fc1(state))\n",
        "        V = self.V(flat1)\n",
        "        A = self.A(flat1)\n",
        "        return V, A\n",
        "        \n",
        "    def Myforward(self, state):\n",
        "        state =state.double()\n",
        "        flat1 = F.relu(self.fc2(state)).to(self.device)\n",
        "        V = self.V(flat1)\n",
        "        A = self.Ao(flat1)\n",
        "\n",
        "        return V, A\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, n, ActionSpace, n_actions, gamma, epsilon, lr, input_dims,\n",
        "                  batch_size, mem_size=100_000,eps_min=0.01, eps_dec=5e-7,\n",
        "                 replace=1000, chkpt_dir='tmp/dueling_ddqn'):\n",
        "\n",
        "        self.NumOfSwitches = n;\n",
        "        self.action_space = ActionSpace;\n",
        "        self.numOfActions =n_actions;\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.lr = lr\n",
        "        self.n_actions = n_actions\n",
        "        self.input_dims = input_dims\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_min = eps_min\n",
        "        self.eps_dec = eps_dec\n",
        "        self.replace_target_cnt = replace\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        #self.action_space = [i for i in range(self.n_actions)]\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        self.memory = ReplayBuffer(mem_size, self.input_dims) # , self.numOfActions\n",
        "\n",
        "        self.q_eval = DuelingDeepQNetwork(self.lr, self.n_actions,\n",
        "                                   input_dims=self.input_dims,\n",
        "                                   name='lunar_lander_dueling_ddqn_q_eval',\n",
        "                                   chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "        self.q_next = DuelingDeepQNetwork(self.lr, self.n_actions,\n",
        "                                   input_dims=self.input_dims,\n",
        "                                   name='lunar_lander_dueling_ddqn_q_next',\n",
        "                                   chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:#>############################\n",
        "            action = []\n",
        "            actiontest=[]\n",
        "            # observation = DictToTensorList(observation)#Added from dqn\n",
        "            # state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
        "            # print(f\"choose_action observation= {observation}\")\n",
        "            count=0\n",
        "            TotalObsrevation = DictToTensorList(observation)\n",
        "            for i in range(0,self.NumOfSwitches):\n",
        "              # partOfObsrevation = DictToTensorList(observation[i:i+1])\n",
        "              partOfObsrevation = TotalObsrevation[i+i:i+i+2]\n",
        "              # print(f\"choose_action partOfObsrevation= {partOfObsrevation}\")\n",
        "              state = T.tensor(partOfObsrevation,dtype=T.float).to(self.q_eval.device)\n",
        "              # print(f\"choose_action state= {state}\")\n",
        "              _,advantage = self.q_eval.Myforward(state)\n",
        "              # print(f\"choose_action advantage= {advantage}\")\n",
        "              # print(f\"choose_action action= {action}\")\n",
        "              action.append({Switches_names[count]:T.argmax(advantage).item()})\n",
        "              count+=1\n",
        "            # print(f\"choose_action action= {action}\")\n",
        "            \n",
        "        else:\n",
        "            #action = np.random.choice(self.action_space)\n",
        "            action = randomchoice(self.action_space)\n",
        "            # print(f\"choose_action randomaction= {action}\")\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        self.memory.store_transition(state, action, reward, state_, done)\n",
        "\n",
        "    def replace_target_network(self):\n",
        "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
        "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
        "\n",
        "    def decrement_epsilon(self):\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                        if self.epsilon > self.eps_min else self.eps_min\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_eval.save_checkpoint()\n",
        "        self.q_next.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_eval.load_checkpoint()\n",
        "        self.q_next.load_checkpoint()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.q_eval.optimizer.zero_grad()\n",
        "\n",
        "        self.replace_target_network()\n",
        "\n",
        "        state, action, reward, new_state, done = \\\n",
        "                                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        \n",
        "        states = T.tensor(state).to(self.q_eval.device)\n",
        "        # print(f\"(learn) len action = {len(action[0])}\")\n",
        "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
        "        dones = T.tensor(done).to(self.q_eval.device)\n",
        "        actions = T.tensor(action).to(self.q_eval.device)\n",
        "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
        "        indices = np.arange(self.batch_size)\n",
        "\n",
        "        print(f\"(learn) state len(x,y)= {len(state),len(state[0])}\")\n",
        "        V_s, A_s = self.q_eval.forward(states)\n",
        "        V_s_, A_s_ = self.q_next.forward(states_)\n",
        "        V_s_eval, A_s_eval = self.q_eval.forward(states_)\n",
        "        # print(f\"(Learn)q_eval states_ A_s = {A_s}\")\n",
        "\n",
        "        indices = [indices] #O\n",
        "        indices = T.tensor(indices) #O\n",
        "        indices = indices.H #O\n",
        "        q_pred = T.add(V_s,(A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions.tolist()]\n",
        "        q_next = T.add(V_s_,(A_s_ - A_s_.mean(dim=1, keepdim=True)))\n",
        "        q_eval = T.add(V_s_eval, (A_s_eval - A_s_eval.mean(dim=1,keepdim=True)))\n",
        "        # print(f\"q_eval = {q_eval}\")\n",
        "        print(f\"q_eval len = {len(q_eval),len(q_eval[0])}\")\n",
        "        max_actions = T.argmax(q_eval, dim=1)\n",
        "        print(f\"max_actions = {max_actions}\")\n",
        "        # max_actions=[]\n",
        "        max = sum(q_eval[0])\n",
        "        global m \n",
        "        for i in range(len(q_eval[0])):\n",
        "          if sum(q_eval[i]) >= max:\n",
        "            max = sum(q_eval[i])\n",
        "            m = i\n",
        "        # max_actions.append([T.argmax(i, dim=-1) for i in max])          \n",
        "        # print(f\"m = {m}\")\n",
        "        # max_actions.append([T.argmax(i, dim=-1) for i in q_eval])#may make dim =0\n",
        "        # max = [T.argmax(i, dim=-1) for i in q_eval]\n",
        "        q_next[dones] = 0.0\n",
        "        # print(f\"max len = {len(max)}\")\n",
        "        # print(f\"indices  = {indices}\")\n",
        "        m = actions[m].int()\n",
        "        m = m.tolist()\n",
        "        # m=[m]\n",
        "        # indices = indices.H\n",
        "        print(f\"rewards = {rewards}\")\n",
        "        rewards = rewards.tolist()\n",
        "        rewards = [rewards]\n",
        "        rewards = T.Tensor(rewards).int()\n",
        "        rewards = rewards.H\n",
        "        print(f\"q_next len = {len(q_next),len(q_next[0])}\")\n",
        "        q_target = rewards + self.gamma*q_next[indices,m]\n",
        "        print(f\"q_target len = {q_target.size()}\")\n",
        "        print(f\"q_pred len = {q_pred.size()}\")\n",
        "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
        "        loss.backward()\n",
        "        self.q_eval.optimizer.step()\n",
        "        self.learn_step_counter += 1\n",
        "        self.decrement_epsilon()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "283WOuAHaJ7G"
      },
      "source": [
        "# **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hxIp0aOoaN7j"
      },
      "outputs": [],
      "source": [
        "#Utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def plotLearning(x, latency, load):\n",
        "    y = latency\n",
        "    z = load\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, y, color='orange', linestyle='dashed', linewidth = 2,\n",
        "         marker='X', markerfacecolor='red', markersize=10)\n",
        "    ax.set_ylabel('Latency', color=\"C0\")\n",
        "    plt.show()\n",
        "    #load balance plot\n",
        "    fig, ax2 = plt.subplots()\n",
        "    ax2.plot(x, z, color='green', linestyle='dashed', linewidth = 2,\n",
        "        marker='X', markerfacecolor='blue', markersize=10)\n",
        "    ax2.set_ylabel('Load Balance', color=\"C1\")\n",
        "    plt.show()\n",
        "#SwitchesLatency = [[10.82284513], [14.18113285]]\n",
        "#SwitchesLoad = [843.109006153129, 692.2788237834724]\n",
        "#time = [5, 10]\n",
        "#plotLearning(time,SwitchesLatency,SwitchesLoad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwbJool-uYMo"
      },
      "source": [
        "# **Global Random Function **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n-ud_Nnd5Vtb",
        "outputId": "b36d45ed-78c2-4331-82df-e39bf4ede490"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  choice=[]\\n  for i in sapce:\\n      for j in i:\\n          rnd_indices = np.random.choice(len(j))\\n          choice.append( j[rnd_indices])\\n          #c=np.random.choice(i)\\n          #state.append(c)\\n          #s=np.array(state)\\n          '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#Global Random Function \n",
        "def randomchoice(space):\n",
        "  choice=[]\n",
        "  for i in space:\n",
        "          rnd_choice = np.random.choice(i)\n",
        "          choice.append( rnd_choice)\n",
        "          #c=np.random.choice(i)\n",
        "          #state.append(c)\n",
        "          #s=np.array(state)\n",
        "  return choice\n",
        "  '''\n",
        "StateSpace = [[{0: 9.314551}, {1: 9.314551}, {2: 9.314551}],\n",
        "              [{0: 151.188115},{1: 151.188115}, {2: 151.188115}],\n",
        "              [{0: 131.828622}, {1: 131.828622},{2: 131.828622}],\n",
        "              [{0: 124.998645}, {1: 124.998645}, {2: 124.998645}],\n",
        "              [{0: 159.46923}, {1: 159.46923}, {2: 159.46923}],\n",
        "              [{0: 324.026864}, {1: 324.026864}, {2: 324.026864}],\n",
        "              [{0: 87.957416}, {1: 87.957416}, {2: 87.957416}],\n",
        "              [{0: 328.540483}, {1: 328.540483}, {2: 328.540483}],\n",
        "              [{0: 461.294549}, {1: 461.294549}, {2: 461.294549}],\n",
        "              [{0: 33.413244}, {1: 33.413244}, {2: 33.413244}],\n",
        "              [{0: 121.985259}, {1: 121.985259}, {2: 121.985259}],\n",
        "              [{0: 607.703116}, {1: 607.703116}, {2: 607.703116}]]\n",
        "              '''\n",
        "#print(f\"random choices : {randomchoice(StateSpace)}\")\n",
        "'''\n",
        "  choice=[]\n",
        "  for i in sapce:\n",
        "      for j in i:\n",
        "          rnd_indices = np.random.choice(len(j))\n",
        "          choice.append( j[rnd_indices])\n",
        "          #c=np.random.choice(i)\n",
        "          #state.append(c)\n",
        "          #s=np.array(state)\n",
        "          '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrb2XGhwnFnd"
      },
      "source": [
        "# **Custom Environment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9glhmVvlmO-W"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Jul 20 21:49:02 2022\n",
        "\n",
        "@author: hp\n",
        "\"\"\"\n",
        "#Custom Environment\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "#from reward import LoadBalance\n",
        "#from DQN_Pytorch import Agent\n",
        "#from utils import plotLearning\n",
        "#from reward import PacketInAverageLatency\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# parameters taken from the second paper\n",
        "Minibatch_size = 64\n",
        "Replay_buffer_size = 1000_000\n",
        "#Learning_start = 3000  \n",
        "Target_update_frequency = 10_000\n",
        "Learning_rate = 0.0001\n",
        "\n",
        "# parameters taken from videos\n",
        "\n",
        "discount_rate = 0.95  #discont factor\n",
        "Epsilon_start = 1\n",
        "Epsilon_end = 0.02 #used in colab\n",
        "Epsilon_decay = 10_000 #used in colab\n",
        "mem_size = 100_000 #used in colab\n",
        "\n",
        "#mine \n",
        "\n",
        "theta = 0.3\n",
        "theta_ = theta\n",
        "action_memory = np.zeros(mem_size,dtype=np.int64)\n",
        "target_action_memory = action_memory\n",
        "\n",
        "class SDNEnv(Env):\n",
        "    def __init__(self,n,k,StateSpace,ActionSpace):\n",
        "        # Actions we can take, down, stay, up\n",
        "        \n",
        "        self.ClustersNum = k\n",
        "        self.SwitchesNum = n\n",
        "        self.action_space = ActionSpace\n",
        "        # Temperature array\n",
        "        #self.observation_space = Box(low=np.array([0]), high=np.array([10]))\n",
        "        #self.observation_space = Box(low=np.array([0]), high=np.array([10]),dtype= np.float128)\n",
        "        # Set start temp        \n",
        "        self.statespace = StateSpace\n",
        "        # Set shower length\n",
        "        self.length = 64\n",
        "        self.state = randomchoice(StateSpace)\n",
        "    def step(self, action):\n",
        "        # Apply action\n",
        "        \n",
        "        #self.state += action -1 \n",
        "        #c = []\n",
        "        count=0     \n",
        "        NewState = []\n",
        "        #NewState.append([])\n",
        "        '''\n",
        "        if x:\n",
        "            actiontemp=action\n",
        "            action=[]\n",
        "            for i,j in zip(actiontemp , x):\n",
        "                action.append({i:j})\n",
        "               '''\n",
        "        c = action\n",
        "        # print(f\"action step = {action}\")\n",
        "        #print(\"action in step = \"+ str(action))\n",
        "        for i in self.state:            \n",
        "            #c = action\n",
        "            # print(c)\n",
        "            NewState.append({list(c[count].values())[0]: list(i.values())[0]})  #output:[{1: 2222}, {0: 5555}, {1: 7777}, {2: 8888}]\n",
        "            count+=1\n",
        "        # print(f\"step NewState = {NewState}\")\n",
        "        #print(count)\n",
        "        # print(f\"new state {NewState}\")\n",
        "               \n",
        "        '''\n",
        "        for i in self.statespace:\n",
        "            NewState.append(np.random.choice(i))\n",
        "        '''\n",
        "        # Reduce shower length by 1 second\n",
        "        self.length -= 1 \n",
        "        #print(\"length = \"+str(self.length))\n",
        "        # Calculate reward\n",
        "        #count = 0\n",
        "        f = Flows\n",
        "        latency,load = f.LoadBalance(action,self.ClustersNum,NewState)\n",
        "        reward = -((0.5*latency)+(0.5*load))\n",
        "        #print(f\"Reward = {reward}\")\n",
        "        # Check if shower is done\n",
        "\n",
        "        if self.length <= 0: \n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "          \n",
        "          \n",
        "        #done=False\n",
        "        \n",
        "        # Apply temperature noise\n",
        "        #self.state += random.randint(-1,1)\n",
        "        # Set placeholder for info\n",
        "        info = {}\n",
        "        \n",
        "        # Return step information\n",
        "        return NewState, reward, done, info, latency, load\n",
        "    \n",
        "    def render(self):\n",
        "        # Implement viz\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        # Reset shower temperature\n",
        "        #state = []\n",
        "        #for i in self.statespace:\n",
        "          #state.append(np.random.choice(i))\n",
        "        self.state = randomchoice(self.statespace)\n",
        "        # print(f\"reset self.state = {self.state}\")\n",
        "        # Reset shower time\n",
        "        self.length = 64\n",
        "\n",
        "        return self.state\n",
        "\n",
        "\n",
        "class Deployment:\n",
        "    \n",
        "    #env = gym.make('LunarLander-v2')\n",
        "    def run(n,k,StateSpace,ActionSpace):\n",
        "        \n",
        "#        self.n_switches = n_switches\n",
        " #       action = {}\n",
        "  #      for i in range(n_switches):          \n",
        "   #       action.update({i:random.choice(range(1,k+1))})\n",
        "        n_actions = k\n",
        "        #n_actions = n\n",
        "        \n",
        "        agent = Agent(n, ActionSpace, n_actions,gamma=0.95, epsilon=1.0, lr=0.0001,input_dims=[n*2],\n",
        "                      batch_size=64)\n",
        "        \n",
        "       \n",
        "       # scores, eps_history = [], []\n",
        "        SwitchesLatency, SwitchesLoad = [], []\n",
        "        n_games = 100 #on 2nd paper\n",
        "        env = SDNEnv(n,k,StateSpace,ActionSpace)\n",
        "        for i in range(n_games):\n",
        "            print(\"i = \"+str(i))\n",
        "            Latencyscore = 0\n",
        "            Loadscore = 0\n",
        "            done = False\n",
        "            observation = env.reset()\n",
        "            # print(f\"(custom) observation = {observation}\")\n",
        "            j=0\n",
        "            while not done:\n",
        "                # print(\"counter = \"+str(j))\n",
        "                j+=1\n",
        "                #print(\"observation \"+str(observation))\n",
        "                #observation [{0: 9.314551}, {1: 151.188115}, {1: 131.828622}, {1: 124.998645},\n",
        "                #{1: 159.46923}, {0: 324.026864}, {2: 87.957416}, {2: 328.540483}, \n",
        "                #{2: 461.294549}, {2: 33.413244}, {2: 121.985259}, {1: 607.703116}]\n",
        "                #action,x = agent.choose_action(observation)\n",
        "                action = agent.choose_action(observation)\n",
        "                #print(f\"x= {x}\")\n",
        "                # print(\"len of action \"+str(len(action)))\n",
        "                observation_, reward, done, info, latency, load = env.step(action)\n",
        "                #print(\"len of observation_ \"+str(len(observation_)))\n",
        "                #print(f\"state in custom is {observation}\")\n",
        "                agent.store_transition(observation, action, reward, \n",
        "                                        observation_, done)\n",
        "                agent.learn()\n",
        "                observation = observation_\n",
        "                Latencyscore += latency\n",
        "                Loadscore += load\n",
        "            #scores.append(score)\n",
        "                SwitchesLatency.append(latency)\n",
        "                SwitchesLoad.append(load)\n",
        "            \n",
        "            #avg_score = np.mean(scores[-100:])\n",
        "    \n",
        "            print('episode ', i,'latency %.2f' % Latencyscore,\n",
        "                    'load %.2f' % Loadscore)\n",
        "        #x = [i+1 for i in range(n_games)]\n",
        "        #x += 5\n",
        "        #filename = 'lunar_lander.png'\n",
        "        #plotLearning(x, latency, load)\n",
        "        return latency ,load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypNSbtladQw"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2x0KYoHfVWg9",
        "outputId": "e6dc9ac6-c015-4738-c932-9d69cfd1ffc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1\n",
            "i = 0\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -61.6168, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "episode  0 latency 500.46 load 5957.65\n",
            "i = 1\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -61.6168, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -61.6168, -49.9931, -50.5518,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -61.6168, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -61.6168, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f718e7bc3298>:192: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
            "  q_pred = T.add(V_s,(A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions.tolist()]\n",
            "<ipython-input-15-f718e7bc3298>:210: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)\n",
            "  q_next[dones] = 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -49.9931, -50.5518, -61.6168, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -61.6168, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -61.6168, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -61.6168, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -61.6168,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -61.6168, -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -61.6168, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -61.6168, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "rewards = tensor([-49.9931, -49.9931, -49.9931, -49.9931, -61.6168, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -61.6168, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -61.6168,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -49.9931, -49.9931, -61.6168, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -61.6168, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rewards = tensor([-49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -61.6168, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -61.6168, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -49.9931, -49.9931, -50.5518,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -49.9931, -49.9931, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -61.6168, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -49.9931, -50.5518, -50.5518, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518,\n",
            "        -50.5518, -49.9931, -61.6168, -50.5518, -49.9931, -50.5518, -49.9931,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -49.9931,\n",
            "        -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -49.9931,\n",
            "        -49.9931, -49.9931, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "(learn) state len(x,y)= (64, 24)\n",
            "q_eval len = (64, 24)\n",
            "max_actions = tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17])\n",
            "rewards = tensor([-50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518, -50.5518,\n",
            "        -50.5518, -50.5518, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -49.9931, -50.5518,\n",
            "        -49.9931, -50.5518, -50.5518, -49.9931, -49.9931, -49.9931, -50.5518,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -49.9931, -61.6168,\n",
            "        -50.5518, -49.9931, -49.9931, -49.9931, -49.9931, -50.5518, -49.9931,\n",
            "        -50.5518, -49.9931, -50.5518, -50.5518, -50.5518, -50.5518, -50.5518,\n",
            "        -49.9931], dtype=torch.float64)\n",
            "q_next len = (64, 24)\n",
            "q_target len = torch.Size([64, 24])\n",
            "q_pred len = torch.Size([64, 24])\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-21-3b55dac25ab2>\", line 73, in <module>\n",
            "    latency , load = D.run(w,k,StateSpace,ActionSpace)\n",
            "  File \"<ipython-input-18-2f15e19387e3>\", line 174, in run\n",
            "    agent.learn()\n",
            "  File \"<ipython-input-15-f718e7bc3298>\", line 227, in learn\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 744, in getmodule\n",
            "    for modname, module in sys.modules.copy().items():\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed May 11 10:33:39 2022\n",
        "\n",
        "@author: hp\n",
        "\"\"\"\n",
        "#Main\n",
        "#import os\n",
        "import pandas as pd\n",
        "\n",
        "from collections import deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd \n",
        "CUDA_LAUNCH_BLOCKING = \"1\"\n",
        "\n",
        "Switches_names={0:\"ATLAM5\",1:\"ATLAng\",2:\"CHINng\",3:\"DNVRng\",4:\"HSTNng\",5:\"IPLSng\",6:\"KSCYng\",7:\"LOSAng\",8:\"NYCMng\",\n",
        "                9:\"SNVAng\",10:\"STTLng\",11:\"WASHng\"}  \n",
        "\n",
        "#from CustomEnvironment import custom\n",
        "\n",
        "df= pd.read_csv(\"positions.csv\")\n",
        "source = df['source']\n",
        "target = df['target']\n",
        "SwitchesMapping =[]\n",
        "ControllersMapping=[(12,13),(13,14)]\n",
        "for x,y in zip(source,target):\n",
        "    Tuple=(x,y)\n",
        "    SwitchesMapping.append(Tuple)\n",
        "    \n",
        "\n",
        "#The first algorithm\n",
        "number = 0\n",
        "k=3\n",
        "W = w = 12\n",
        "StateSpace = []\n",
        "ActionSpace = []\n",
        "counter = 0\n",
        "df = pd.read_excel('MergeTest.xlsx')\n",
        "traffic = df['traffic']\n",
        "df = df.reset_index()\n",
        "#for item , row in df.iterrows():\n",
        "D = Deployment\n",
        "iterations=0\n",
        "x=0\n",
        "SwitchesLatency, SwitchesLoad, time = [], [], []\n",
        "#prepare the state sequence\n",
        "for i in range(0,len(traffic),W): #change def to traffic \n",
        "    while number < W :\n",
        "        StateSpace.append([]) \n",
        "        ActionSpace.append([])\n",
        "        # action list will be  [[{switch1--> 'ATLAM5':clusternumber -->0},{0:1},{0:2}],\n",
        "        #                        [{switch2-->'ATLAng':clusternumber -->0},{1:1},{2:1}]\n",
        "        #                        [                                            ]...12]\n",
        "        for j in range(k):\n",
        "            ActionSpace[counter].append({list(Switches_names.values())[counter]:j})  #list(Switches_names.values())[counter] instead of counter\n",
        "            \n",
        "            \n",
        "        for controller in range(k):\n",
        "            StateSpace[counter].append({controller:df.loc[number, \"traffic\"]})\n",
        "            \n",
        "        number+=1\n",
        "        counter+=1\n",
        "    #print(f\"action {action}\")\n",
        "    iterations+=1\n",
        "    print(f\"iteration {iterations}\")\n",
        "    #print(f\"StateSpace = {StateSpace}\")\n",
        "    #print(action)   \n",
        "    W+= w\n",
        "    counter=0\n",
        "             \n",
        "    latency , load = D.run(w,k,StateSpace,ActionSpace)\n",
        "    SwitchesLatency.append(latency)\n",
        "    SwitchesLoad.append(load)\n",
        "    x += 5\n",
        "    time.append(x)\n",
        "    #custom(SS,discount_rate,Epsilon_start,Epsilon_end,Epsilon_decay)\n",
        "    #custom(w,k,StateSpace,ActionSpace)\n",
        "    \n",
        "    #print(SS)\n",
        "    StateSpace.clear()\n",
        "    ActionSpace.clear()\n",
        "print(f\"SwitchesLatency = {SwitchesLatency}\")\n",
        "print(f\"SwitchesLoad = {SwitchesLoad}\")\n",
        "print(f\"time = {time}\")\n",
        "\n",
        "plotLearning(time, SwitchesLatency, SwitchesLoad)        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "6l4I-XGFgsp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {\n",
        "    'name': 'Borislav Hadzhiev',\n",
        "    'fruit': 'apple',\n",
        "    'number': 5,\n",
        "    'website': 'bobbyhadz.com',\n",
        "    'topic': 'Python'\n",
        "}\n",
        "q_eval =T.Tensor( [[1,1,1],[2,1,0],[2,2,4]])\n",
        "\n",
        "m = [sum(i).tolist() for i in q_eval]\n",
        "# m = max(m)\n",
        "m = T.Tensor(m)\n",
        "print(m)\n",
        "print(q_eval)\n",
        "max_actions = T.argmax(m, dim=0)\n",
        "max_actions = q_eval[max_actions]\n",
        "print(max_actions)\n"
      ],
      "metadata": {
        "id": "KP3x39-75f_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b = T.randn(4) 64*24 --> 2*4 // indices: 1*64, actions: 64*24, V: 64*1, A:64*3\n",
        "V=T.tensor([[1],\n",
        "            [2],\n",
        "            [3],\n",
        "            [4],[4]])\n",
        "# print(V)\n",
        "A=T.tensor([[1,2,3,2,1],\n",
        "            [4,5,6,6,1],\n",
        "            [7,8,9,9,1],\n",
        "            [10,11,12,13,1],\n",
        "            [1,1,1,1,1]])\n",
        "# print(f\"A = {A}\")\n",
        "ind = T.tensor([1,2,3,4])\n",
        "ind2 = T.tensor([[1,2],\n",
        "                 [3,4],\n",
        "                 [5,6],\n",
        "                 [7,8]])\n",
        "# ind = ind.long()\n",
        "# print(type(ind))\n",
        "actions=T.tensor([[0, 0, 1, 0, 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0, 9, 0, 10, 0, 11, 0]])\n",
        "# actions = actions.int()\n",
        "# x=A - A.mean(dim=1,dtype = float, keepdim=True)\n",
        "# print(f\"A.mean(dim=1,dtype = float, keepdim=True) = {A.mean(dim=1,dtype = float, keepdim=True)}\")\n",
        "# print(x)\n",
        "# print(T.add(V, (A - A.mean(dim=1,dtype = float, keepdim=True))))\n",
        "# print(T.add(V, (A - A.mean(dim=1,dtype = float, keepdim=True)))[[[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3]],\n",
        "                                                                # [[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]]])\n",
        "# x=[[0],[1],[2],[3]]\n",
        "# x=[x]\n",
        "# x=T.tensor(x)\n",
        "# x= x.H\n",
        "indices = np.arange(64)\n",
        "indices = [indices] #O\n",
        "indices = T.tensor(indices) #O\n",
        "indices = indices.H #O\n",
        "# print(A[x,actions])\n",
        "# print(T.add(V, (A - A.mean(dim=1,dtype = float, keepdim=True))))\n",
        "# print(T.add(V, (A - A.mean(dim=1,dtype = float, keepdim=True)))[indices,actions])\n",
        "# all num inside actions must be in range of A num of rows\n",
        "#############################test2\n",
        "# index=[[1],[2],[3]]\n",
        "# act=[[(1,1),(2,2)],[(1,1),(2,2)],[(1,1),(2,2)]]\n",
        "x=T.tensor([1,2,3])\n",
        "x=x.tolist()\n",
        "x=[x]\n",
        "x=T.Tensor(x).int()\n",
        "x=x.H\n",
        "print(x)"
      ],
      "metadata": {
        "id": "7c1auOZv5wNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dd)\n",
        "# print(T.add(bb, dd)[[0,1],[[1,2],[0,1]]])\n",
        "# [0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\n",
        "# print(T.add(bb, dd))\n",
        "a=T.tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
        "      19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35 ,36 ,37 ,38, 39, 40,\n",
        "      41, 42, 43, 44 ,45 ,46 ,47 ,48 ,49 ,50 ,51 ,52 ,53 ,54 ,55 ,56 ,57 ,58 ,59 ,60, 61, 62, 63 ])\n",
        "b=T.tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
        "      19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35 ,36 ,37 ,38, 39, 40,\n",
        "      41, 42, 43, 44 ,45 ,46 ,47 ,48 ,49 ,50 ,51 ,52 ,53 ,54 ,55 ,56 ,57 ,58 ,59 ,60, 61, 62, 63 ])\n",
        "\n",
        "ind =[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
        "      19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35 ,36 ,37 ,38, 39, 40,\n",
        "      41, 42, 43, 44 ,45 ,46 ,47 ,48 ,49 ,50 ,51 ,52 ,53 ,54 ,55 ,56 ,57 ,58 ,59 ,60, 61, 62, 63 ]\n",
        "\n",
        "actions = T.tensor([[ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.],\n",
        "   [ 0.,  0.,  1.,  1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,  0., 11.,  0.]])\n",
        "vs= T.tensor([[-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7840],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274],\n",
        "        [-3.7274]])\n",
        "ass  = T.tensor([[10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.2447,  0.6912, -0.5362,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1],\n",
        "        [10.3163,  0.4484, -0.4993,1,1,1,1,1,1,1,1,1]])\n",
        "# print(actions)\n",
        "# ind = [ind]\n",
        "# ind=T.tensor(ind)\n",
        "# ind= ind.H\n",
        "# act=[]\n",
        "# print(ind)\n",
        "# [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
        "# for i in range(len(ind)):\n",
        "#   ind[i] = float(i)\n",
        "# for i in range(len(actions)):\n",
        "#   for j in range(len(actions[i])):\n",
        "#     # print(actions[i][j])\n",
        "#     actions[i][j]=(float(actions[i][j]))\n",
        "# print(act)\n",
        "actions=T.tensor([[0, 0, 1, 0, 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0, 9, 0, 10, 0, 11, 0]])\n",
        "indices = np.arange(64)\n",
        "indices = [indices] #O\n",
        "indices = T.tensor(indices) #O\n",
        "indices = indices.H #O\n",
        "test = T.add(vs, (ass - ass.mean(dim=1,dtype=float, keepdim=True)))[indices,actions.tolist()]\n",
        "test = vs + ass[indices,actions.tolist()]\n",
        "# print(test.size())\n",
        "a=[[1,1,1],[2,2,2]]\n",
        "print(len(a[0]))"
      ],
      "metadata": {
        "id": "8gdtEyKKq4tN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "OteoOuPvZEyN",
        "7jchUeIAbfjb",
        "iGtwZaz86myz",
        "XXERaiawZvKl",
        "CwbJool-uYMo"
      ],
      "provenance": [],
      "mount_file_id": "1vrBnDrouUDfg36woA9OdO_n3rqo_sLHf",
      "authorship_tag": "ABX9TyMS8ceVFMlq0EfUK2vZ2suP",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}